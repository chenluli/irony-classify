{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from json import JSONDecoder\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data\"\n",
    "trainA_file = os.path.join(data_folder, 'trainA.json')\n",
    "trainB_file = os.path.join(data_folder, 'trainB.json')\n",
    "test_file = os.path.join(data_folder, 'test.json')\n",
    "\n",
    "train_feature_f = os.path.join(data_folder, 'task3_train_feature.txt')\n",
    "test_feature_f = os.path.join(data_folder, 'task3_test_feature.txt')\n",
    "\n",
    "word2idx_f = os.path.join(data_folder, 'word2idx.json')\n",
    "pos2idx_f = os.path.join(data_folder, 'pos2idx.json')\n",
    "word_embeds_f = os.path.join(data_folder, 'word_embedding.npy')\n",
    "pos_embeds_f = os.path.join(data_folder, 'pos_embedding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        dataList=data['data']\n",
    "    return dataList\n",
    "\n",
    "def load_seq_feats(filename):\n",
    "    with open(filename,'r')as f:\n",
    "        test_feature=f.read()\n",
    "    test_feature=JSONDecoder().decode(test_feature)\n",
    "    return test_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train: 3450 #valud: 384 #test: 784\n"
     ]
    }
   ],
   "source": [
    "trainA = load_data(trainA_file)[:3450]\n",
    "validA = load_data(trainA_file)[3450:]\n",
    "trainB = load_data(trainB_file)[:3450]\n",
    "validB = load_data(trainB_file)[3450:]\n",
    "test = load_data(test_file)\n",
    "print(\"#train:\", len(trainA),\"#valud:\", len(validA),\"#test:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = load_seq_feats(train_feature_f)\n",
    "test_feature = load_seq_feats(test_feature_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load word and pos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(fname):\n",
    "    with open(fname, 'r') as file:\n",
    "        dictionary = json.load(file)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 12656\n"
     ]
    }
   ],
   "source": [
    "word2idx = load_dict(word2idx_f)\n",
    "pos2idx = load_dict(pos2idx_f)\n",
    "vocab_size = len(word2idx)\n",
    "print(\"vocab size:\", vocab_size)\n",
    "\n",
    "word_embedding = np.load(word_embeds_f)\n",
    "pos_embedding = np.load(pos_embeds_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12656, 700) (28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(word_embedding.shape, pos_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding  \n",
    "$P E_{2 i}(p)=\\sin \\left(p / 10000^{2 i / d_{model}}\\right)$   \n",
    "$P E_{2 i+1}(p)=\\cos \\left(p / 10000^{2 i / d_{model}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回的是x+pe\n",
    "def positional_enc(x, dim_model, max_len=5000):\n",
    "    base = 10000\n",
    "    sentence_len = x.size(1)\n",
    "    pe_vec = torch.zeros(max_len, dim_model)\n",
    "    p = torch.arange(0., max_len).unsqueeze(1)\n",
    "    frac = torch.exp(torch.arange(0., dim_model, 2) * -(math.log(10000.0) / dim_model)) \n",
    "    pe_vec[:,0::2] = torch.sin(p)\n",
    "    pe_vec[:,1::2] = torch.cos(p)\n",
    "    pe_vec = pe_vec.unsqueeze(0)\n",
    "    return x.float() + pe_vec[:,:sentence_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention\n",
    "scaled dot product attention:\n",
    " $\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None, dropout=0.1):\n",
    "    dim_key = query.size(-1)\n",
    "    attn = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(dim_key)\n",
    "    if mask is not None:\n",
    "        attn = scores.masked_fill(mask == 0, -1e9)\n",
    "    attn_weights = F.softmax(attn, dim = -1)\n",
    "    if dropout is not None:\n",
    "        attn_weights = dropout(attn_weights)\n",
    "    return torch.matmul(attn_weights, value), attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self attention: K=V=Q  \n",
    "each word in the sentence needs to undergo Attention computation, to capture the internal structure of the sentence\n",
    "\n",
    "Multi-head Attention: query, key, and value first go through a linear transformation and then enters into Scaled-Dot Attention. Here, the attention is calculated h times, which allows the model to learn relevant information in different representative child spaces.  \n",
    "When #head=1, it becomes a original self-attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads, dim_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # make sure input word embedding dimension divides by the number of desired heads\n",
    "        assert dim_model % num_heads == 0\n",
    "        # assume dim of key,query,values are equal\n",
    "        self.dim_qkv = dim_model // num_heads\n",
    "        \n",
    "        self.dim_model = dim_model\n",
    "        self.num_h = num_heads\n",
    "        self.w_q = nn.Linear(dim_model, dim_model) # self.w_qs = nn.Linear(d_model, n_head * d_k) \n",
    "        self.w_k = nn.Linear(dim_model, dim_model) \n",
    "        self.w_v = nn.Linear(dim_model, dim_model)\n",
    "        \n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            \n",
    "        n_batch = query.size(0)\n",
    "#         residual = query\n",
    "        \n",
    "        # linear projections: dim_model => num_h x dim_k \n",
    "        query = self.w_q(query).view(n_batch, -1, self.num_h, self.dim_qkv)\n",
    "        key = self.w_k(key).view(n_batch, -1, self.num_h, self.dim_qkv)\n",
    "        value = self.w_v(value).view(n_batch, -1, self.num_h, self.dim_qkv)\n",
    "        \n",
    "        # Apply attention on all the projected vectors in batch \n",
    "        x, self.attn = scaled_dot_product_attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # Concat(head1, ..., headh) \n",
    "        x = x.transpose(1, 2).contiguous().view(n_batch, -1, self.num_h * self.dim_qkv)\n",
    "        \n",
    "        x = nn.Linear(dim_model, dim_model, bias=False)(x)\n",
    "#         x = self.layer_norm(x + residual)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position-wise feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu # bert uses gelu instead\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add & Norm\n",
    "`Residual connection`是对于较为深层的神经网络有比较好的作用，比如网络层很深时，数值的传播随着weight不断的减弱，`Residual connection`是从输入的部分，连到它输出层的部分，把输入的信息原封不动copy到输出的部分，减少信息的损失。\n",
    "`layer-normalization`这种归一化层是为了防止在某些层中由于某些位置过大或者过小导致数值过大或过小，对神经网络梯度回传时有训练的问题，保证训练的稳定性。基本在每个子网络后面都要加上`layer-normalization`、加上`Residual connection`，加上这两个部分能够使深层神经网络训练更加顺利。  \n",
    "(本实验中也许不需要)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, size, dropout, eps=1e-6):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(size))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(size))\n",
    "        self.eps = eps\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        x = x.float()\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        norm = self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "        return x + self.dropout(sublayer(norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "self-attention layers: all of the keys, values and queries come from the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一层Encoder: self-atten --> add&norm --> feed-forward --> add&norm\n",
    "# 浅层网络可以去掉add&norm层？\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, attention, feed_forward, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.feed_forward = feed_forward\n",
    "        self.self_atten = attention\n",
    "        self.add_norm_1 = AddNorm(size, dropout)\n",
    "        self.add_norm_2 = AddNorm(size, dropout)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        output = self.add_norm_1(x, lambda x: self.self_atten(x, x, x, mask))\n",
    "        output = self.add_norm_2(output, self.feed_forward)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)]) # clone the layer for N times\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(nn.Module):\n",
    "    def __init__(self,n_input,n_out):\n",
    "        super(SoftMax,self).__init__()\n",
    "        self.fc = nn.Linear(n_input,n_out)\n",
    "        self.softmax = nn.LogSoftmax(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        y = self.softmax(x)\n",
    "#         print(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single task\n",
    "# embeding --> encoder --> linear --> softmax\n",
    "class SelfAttenClassifier(nn.Module):\n",
    "    def __init__(self, encoder, classifier):\n",
    "        super(SelfAttenClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, input_embeds):\n",
    "        batch_size = input_embeds.size(1)\n",
    "        encoder_out = self.encoder(input_embeds)\n",
    "        # Simply average the final sequence position representations to create a fixed size \"sentence representation\".\n",
    "#         sentence_representation = tf.reduce_mean(encoder_output, axis=1)    # [batch_size, model_dim]\n",
    "        feats = encoder_out.sum(dim=1)\n",
    "#         print(encoder_out.size(), feats.size())\n",
    "        outputs = self.classifier(feats)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TODO concatenate sentence embedding before classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TODO multi-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize sequences as inputs \n",
    "def seq_to_tensor(raw_sample,dim_model=728):\n",
    "    seq_embed = torch.tensor([np.concatenate([word_embedding[word2idx[w]],pos_embedding[pos2idx[raw_sample[\"pos\"][i]]]]) \n",
    "                       for i,w in enumerate(raw_sample[\"word\"])])\n",
    "    return seq_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IronyDataset(Dataset):\n",
    "    def __init__(self, raw_data, transform=None):\n",
    "        self.data = raw_data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        label = self.data[index][\"label\"]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = IronyDataset(trainA, seq_to_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load and pading sequence batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic padding: seqeuences are padded to the maximum length of mini-batch sequences\n",
    "def collate_fn(batch):\n",
    "    sorted_batch = sorted(batch, key=lambda x: x[0].size(0), reverse=True)\n",
    "    sequences = [x[0] for x in sorted_batch]\n",
    "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "    lengths = torch.LongTensor([len(x) for x in sequences])\n",
    "    labels = torch.LongTensor(list(map(lambda x: x[1], sorted_batch)))\n",
    "    return sequences_padded, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(model,loader):\n",
    "    model.eval()\n",
    "    num_corrects = 0\n",
    "    for data in loader:\n",
    "        x, y, lengths = batch\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "#         print(torch.max(pred, 1)[1].view(y.size()).data)\n",
    "#         print(y.data)\n",
    "        num_corrects += (torch.max(pred, 1)[1].view(y.size()).data == y.data).sum()\n",
    "    return num_corrects.item() / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        x, y, lengths = batch\n",
    "        optimizer.zero_grad()\n",
    "        # !!TOCHECK add positional encoding here or other place??\n",
    "        x = positional_enc(x, dim_model)\n",
    "        out = model(x)\n",
    "        loss = loss_func(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_layers = 2\n",
    "dim_model=728 # equal to the dim of word embeddings\n",
    "num_heads=8 # dim_model % num_heads == 0\n",
    "d_ff=2912\n",
    "dropout=0.1\n",
    "\n",
    "# c = copy.deepcopy\n",
    "attn_layer = MultiHeadedAttention(num_heads, dim_model, dropout)\n",
    "ff_layer = PositionwiseFeedForward(dim_model, d_ff, dropout)\n",
    "\n",
    "num_class = 2\n",
    "model = SelfAttenClassifier(\n",
    "    encoder = Encoder(EncoderLayer(dim_model,attn_layer,ff_layer),num_encoder_layers),\n",
    "    classifier = SoftMax(dim_model,num_class),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Acc: 0.61768116, Duration: 198.18\n",
      "Epoch: 002, Acc: 0.51681159, Duration: 202.29\n",
      "Epoch: 003, Acc: 0.56115942, Duration: 199.98\n",
      "Epoch: 004, Acc: 0.52695652, Duration: 180.62\n",
      "Epoch: 005, Acc: 0.51101449, Duration: 179.93\n",
      "Epoch: 006, Acc: 0.52376812, Duration: 183.29\n",
      "Epoch: 007, Acc: 0.50086957, Duration: 178.35\n",
      "Epoch: 008, Acc: 0.49594203, Duration: 179.43\n",
      "Epoch: 009, Acc: 0.58956522, Duration: 189.67\n",
      "Epoch: 010, Acc: 0.55043478, Duration: 185.93\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "import time\n",
    "loss_function = F.nll_loss \n",
    "\n",
    "time_p, tr_acc_array, ts_acc, loss_p = [], [], [], []\n",
    "epochs = 10\n",
    "# running epoches\n",
    "for epoch in range(1, epochs + 1):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_start = time.perf_counter()\n",
    "\n",
    "        train_loss = train(model, train_loader, loss_function,optimizer)\n",
    "        train_acc = binary_acc(model, train_loader)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_end = time.perf_counter()\n",
    "        time_p.append(t_end)\n",
    "        loss_p.append(train_loss)\n",
    "        tr_acc_array.append(train_acc)\n",
    "\n",
    "        print('Epoch: {:03d}, Acc: {:.8f}, Duration: {:.2f}'.format(\n",
    "            epoch, train_acc, t_end - t_start))\n",
    "\n",
    "# save model params\n",
    "torch.save(model.state_dict(), 'taskA_transformer_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "color = cm.viridis(0.7)\n",
    "f, ax = plt.subplots(1,1)\n",
    "epoches = [i for i in range(len(loss_p))]\n",
    "ax.plot(epoches, loss_p, color=color)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('epoches')\n",
    "ax.set_ylabel('loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TaskB\n",
    "Since the data for taskB is imbalanced, add a weight for the loss of each sample according to its label during the training time:  \n",
    "{label:weight}={0.0: 1.9907674552798615, 1.0: 2.7555910543130993, 2.0:12.23404255319149, 3.0: 18.852459016393443}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_acc(model, loader):\n",
    "    model.eval()\n",
    "    total_acc = 0\n",
    "    for batch in loader:\n",
    "        x, y = batch.text, batch.label - 1\n",
    "        out = model(x)\n",
    "        total_acc += \n",
    "    return total_acc / len(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SelfAttenClassifier(\n",
    "    encoder = Encoder(EncoderLayer(dim_model,attn_layer,ff_layer),num_encoder_layers),\n",
    "    classifier = SoftMax(dim_model,num_class=4),\n",
    "    )\n",
    "\n",
    "weight = torch.tensor([1.9907674552798615, 2.7555910543130993, 12.23404255319149, 18.852459016393443])\n",
    "loss_function = F.nll_loss(weight)\n",
    "\n",
    "time_p, tr_acc_array, ts_acc, loss_p = [], [], [], []\n",
    "epochs = 10\n",
    "# running epoches\n",
    "for epoch in range(1, epochs + 1):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_start = time.perf_counter()\n",
    "\n",
    "        train_loss = train(model, train_loader, loss_function,optimizer)\n",
    "        train_acc = binary_acc(model, train_loader)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_end = time.perf_counter()\n",
    "        time_p.append(t_end)\n",
    "        loss_p.append(train_loss)\n",
    "        tr_acc_array.append(train_acc)\n",
    "\n",
    "        print('Epoch: {:03d}, Acc: {:.8f}, Duration: {:.2f}'.format(\n",
    "            epoch, train_acc, t_end - t_start))\n",
    "\n",
    "# save model params\n",
    "torch.save(model.state_dict(), 'taskA_transformer_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,1)\n",
    "epoches = [i for i in range(len(loss_p))]\n",
    "ax.plot(epoches, loss_p, color=color)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('epoches')\n",
    "ax.set_ylabel('loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_heatmap(model, dialog_vocab, candidate_vocab, memory, query, label=None):\n",
    "    \n",
    "    pred, attn = model(Variable(memory), Variable(query))\n",
    "    \n",
    "    attn = attn.squeeze(0).data.numpy()\n",
    "    \n",
    "    y_labels = []\n",
    "    pad = dialog_vocab.word_to_index('<pad>')\n",
    "    for row in memory.squeeze(0).numpy():\n",
    "        row = row.tolist()\n",
    "        end = len(row)\n",
    "        if pad in row:\n",
    "            end = row.index(pad)\n",
    "        y_labels.append(vec2sent(dialog_vocab, row[2:end]))\n",
    "    \n",
    "    ax = plt.axes()\n",
    "    ax.set_title('Attention Weights per Memory Hops')\n",
    "    ax = sns.heatmap(attn, linewidths=.5, square=True, yticklabels=y_labels, ax=ax, cmap=sns.color_palette(\"Blues\"))\n",
    "    ax.set(xlabel='Hops', ylabel='Memory Contents')\n",
    "    plt.show()\n",
    "    \n",
    "    if label:\n",
    "        print('True label: ', candidate_vocab.index_to_word(label))\n",
    "        print('Prediction: ', candidate_vocab.index_to_word(torch.max(pred.data, 1)[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全self-attention层可能存在的问题：\n",
    "- only capture the inner structure of a sentence, the relations between sentence parts and classification are not captured directly.\n",
    "- position information is not sufficiently modeled.\n",
    "\n",
    "因此经常self-attention层会和RNN/LSTM结合使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "wirte predicted labels for test data:(one sample a line): label+\\t+orinignal word list\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks)\n",
    "- [Multi-head Self Attention for Text Classification](https://www.kaggle.com/fareise/multi-head-self-attention-for-text-classification)\n",
    "\n",
    "--代码参考--\n",
    "- [BERT-pytorch](https://github.com/codertimo/BERT-pytorch]\n",
    "- [Variable-sized mini-batches and why PyTorch is good for your health](https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "env3.5",
   "language": "python",
   "name": "env3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
